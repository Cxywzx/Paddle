# 动态神经网络的实现

动态网络是目前神经网络框架的前沿课题。动态神经网络的优势解决了普通神经网络框架的一个重要问题，**神经网络的定义和计算是分离的**。即普通神经网络框架的计算步骤是，先定义一个神经网络的计算图，再使用计算引擎计算这个计算图。而动态神经网络的特点是，直接对每个操作求值，隐式的定义计算图，从而再对这个隐式的计算图反向传播。

常见的使用方式为:


```python
x = paddle.dyn.data(type=DenseVector(784))
x.fill([0.058, 0.548, ...])

y = paddle.dyn.data(type=Integer(10))
y.fill(9)

hidden = paddle.dyn.fc(input=y, size=200)

# You can use hidden.npvalue() to get this layer's value now.

prediction = paddle.dyn.fc(input=hidden, size=10, act=Softmax())

cost = paddle.dyn.classification_cost(input=prediction, label=y)

if cost.npvalue() < 0.001:
	cost *= 100 # scale up cost if cost is little, just a demo for dynamic network.

print 'Cost = ', cost.npvalue()

cost.backward()
parameters.update()
```

## 动态神经网络解决的问题

动态神经网络只有神经网络的计算步骤，而隐藏了神经网络的定义步骤。他解决的问题是:

* 可以任意的在计算过程中添加非线性的操作，例如`if`。并且对于不同的数据，神经网络的计算图可以不同。例如 树形神经网络

// TODO(qijun): Complete this docs

TBD

## 动态神经网络的实现思路

TBD

## 动态神经网络对神经网络框架的要求

TBD
