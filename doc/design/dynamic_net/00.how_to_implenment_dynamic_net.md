# 动态神经网络的实现

动态网络是目前神经网络框架的前沿课题。动态神经网络的优势解决了普通神经网络框架的一个重要问题，**神经网络的定义和计算是分离的**。即静态神经网络框架的计算步骤是，先定义一个神经网络的计算图，再使用计算引擎计算这个计算图。而动态神经网络的特点是，直接对每个操作求值，隐式的定义计算图，从而再对这个隐式的计算图反向传播。

常见的使用方式为:


```python
x = paddle.dyn.data(type=DenseVector(784))
x.fill([0.058, 0.548, ...])

y = paddle.dyn.data(type=Integer(10))
y.fill(9)

hidden = paddle.dyn.fc(input=x, size=200)

# You can use hidden.npvalue() to get this layer's value now.

prediction = paddle.dyn.fc(input=hidden, size=10, act=Softmax())

cost = paddle.dyn.classification_cost(input=prediction, label=y)

if cost.npvalue() < 0.001:
	cost *= 100 # scale up cost if cost is little, just a demo for dynamic network.

print 'Cost = ', cost.npvalue()

cost.backward()
parameters.update()
```

## 动态神经网络解决的问题

动态神经网络只有神经网络的计算步骤，而隐藏了神经网络的定义步骤，用户可以为每一个sample或者batch定义一个不同的网络。相对于静态神经网络而言，动态神经网络解决了以下几个问题：

* 可以任意的在计算过程中添加复杂的控制逻辑，例如迭代，递归，条件选择等，这些控制逻辑都可以由host language(C++/Python)来实现。
* 可以支持更复杂的数据类型，并且对于不同的数据，神经网络的计算图可以不同。
* 动态神经网络的执行过程就是其定义过程，用户可以对神经网络中的参数，中间结果等信息直接求值，方便debug的过程。


## 动态神经网络的实现思路

动态神经网络计算图的定义是隐式的，其设计哲学可以参考一些autograd库(例如https://github.com/HIPS/autograd)。具体实现思路如下：


1. 对于每一个sample，用户使用layer的组合来定义神经网络结构。每个sample都拥有一个graph结构来记录该sample的计算图。
2. graph中包含每一层layer的信息，包括输入数据来源，该层layer进行的操作，输出数据大小等。新连接上的layer的相关信息会被持续追加到graph中。
3. layer的求值操作是lazy的，直到用户显式的调用value()方法，graph中记录的计算图才会被execute engine真正执行，计算得到该层layer的输出结果。通常情况下执行forward()操作时会对网络进行求值。
4. 用户可以在组合layer的时候加入控制逻辑，被选择的分支信息也会记录到graph中。
5. 在进行backward()操作时，graph的execute engine会根据记录的计算图执行求导操作，计算梯度。




## 动态神经网络对神经网络框架的要求

* 最核心的要求就是构建计算图的过程要足够轻量，后端使用C++来实现，并且考虑设计特定的内存/显存 管理策略。前端的Python wrapper也要足够小，可以直接使用后端C++提供的接口。

* 考虑到layer的求值是lazy的，可以使用表达式模板对计算过程进行优化。

* 考虑对不同大小数据/不同网络结构 组batch进行训练。在动态网络中，每一个sample都拥有自己的计算图，相比于静态网络，在GPU上进行并行操作是比较困难的。
